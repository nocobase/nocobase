---
pkg: "@nocobase/plugin-ai"
---
:::tip Повідомлення про переклад ШІ
Ця документація була автоматично перекладена штучним інтелектом.
:::



# Текстовий чат

## Вступ

Використовуючи LLM-вузол у робочому процесі, ви можете розпочати розмову з онлайн-сервісом LLM, залучаючи можливості великих моделей для допомоги у виконанні низки бізнес-процесів.

![](https://static-docs.nocobase.com/202503041012091.png)

## Створення LLM-вузла

Оскільки розмови з LLM-сервісами часто займають багато часу, LLM-вузол можна використовувати лише в асинхронних робочих процесах.

![](https://static-docs.nocobase.com/202503041013363.png)

## Вибір моделі

Спершу оберіть підключений LLM-сервіс. Якщо жоден LLM-сервіс ще не підключено, вам потрібно спочатку додати його конфігурацію. Дивіться: [Керування LLM-сервісами](/ai-employees/quick-start/llm-service)

Після вибору сервісу програма спробує отримати список доступних моделей з LLM-сервісу для вашого вибору. Деякі онлайн-сервіси LLM можуть мати API для отримання моделей, які не відповідають стандартним протоколам API; у таких випадках користувачі також можуть ввести ID моделі вручну.

![](https://static-docs.nocobase.com/202503041013084.png)

## Налаштування параметрів виклику

Ви можете налаштувати параметри для виклику LLM-моделі за потреби.

![](https://static-docs.nocobase.com/202503041014778.png)

### Response format

Варто звернути увагу на налаштування **Response format**. Цей параметр використовується для вказівки великій моделі формату її відповіді, яка може бути текстовою або JSON. Якщо ви обрали режим JSON, зверніть увагу на наступне:

- Відповідна LLM-модель повинна підтримувати виклик у режимі JSON. Крім того, користувачеві потрібно явно вказати LLM у промпті відповідати у форматі JSON, наприклад: "Tell me a joke about cats, respond in JSON with \`setup\` and \`punchline\` keys". В іншому випадку відповіді може не бути, що призведе до помилки \`400 status code (no body)\`.
- Відповідь буде рядком JSON. Користувачеві потрібно розпарсити її за допомогою можливостей інших вузлів робочого процесу, перш ніж використовувати її структурований вміст. Ви також можете скористатися функцією [Структурований вивід](/ai-employees/workflow/nodes/llm/structured-output).

## Повідомлення

Масив повідомлень, що надсилаються LLM-моделі, може містити набір історичних повідомлень. Повідомлення підтримують три типи:

- System (Система) – Зазвичай використовується для визначення ролі та поведінки LLM-моделі в розмові.
- User (Користувач) – Вміст, введений користувачем.
- Assistant (Асистент) – Вміст, на який відповідає модель.

Для повідомлень користувача, за умови підтримки моделлю, ви можете додати кілька частин вмісту в одному промпті, що відповідає параметру \`content\`. Якщо модель, яку ви використовуєте, підтримує параметр \`content\` лише у вигляді рядка (до цієї категорії належить більшість моделей, що не підтримують мультимодальні розмови), будь ласка, розділіть повідомлення на кілька промптів, кожен з яких містить лише одну частину вмісту. Таким чином, вузол надішле вміст як рядок.

![](https://static-docs.nocobase.com/202503041016140.png)

У вмісті повідомлень ви можете використовувати змінні для посилання на контекст робочого процесу.

![](https://static-docs.nocobase.com/202503041017879.png)

## Використання вмісту відповіді LLM-вузла

Ви можете використовувати вміст відповіді LLM-вузла як змінну в інших вузлах.

![](https://static-docs.nocobase.com/202503041018508.png)