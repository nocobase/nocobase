---
pkg: "@nocobase/plugin-ai"
---
:::tip Уведомление о переводе ИИ
Эта документация была автоматически переведена ИИ.
:::



# Текстовый чат

## Введение

Используя LLM-узел в **рабочем процессе**, вы можете начать диалог с онлайн-сервисом LLM, задействуя возможности больших моделей для автоматизации различных бизнес-процессов.

![](https://static-docs.nocobase.com/202503041012091.png)

## Создание LLM-узла

Поскольку взаимодействие с LLM-сервисами обычно занимает много времени, LLM-узел можно использовать только в асинхронных **рабочих процессах**.

![](https://static-docs.nocobase.com/202503041013363.png)

## Выбор модели

Сначала выберите подключенный LLM-сервис. Если вы ещё не подключили ни один LLM-сервис, вам потребуется сначала добавить его конфигурацию. Подробнее: [Управление LLM-сервисами](/ai-employees/quick-start/llm-service)

После выбора сервиса приложение попытается получить список доступных моделей из LLM-сервиса, чтобы вы могли выбрать нужную. Некоторые онлайн-сервисы LLM могут использовать API для получения моделей, которые не соответствуют стандартным протоколам. В таких случаях вы также можете ввести ID модели вручную.

![](https://static-docs.nocobase.com/202503041013084.png)

## Настройка параметров вызова

Вы можете настроить параметры для вызова LLM-модели по мере необходимости.

![](https://static-docs.nocobase.com/202503041014778.png)

### Формат ответа (Response format)

Особое внимание стоит уделить настройке **Формат ответа (Response format)**. Эта опция используется для указания большой модели, в каком формате должен быть её ответ: текст или JSON. Если вы выбрали режим JSON, учтите следующее:

- Соответствующая LLM-модель должна поддерживать вызов в режиме JSON. Кроме того, вам необходимо явно указать LLM в промпте, чтобы она отвечала в формате JSON, например: "Tell me a joke about cats, respond in JSON with \`setup\` and \`punchline\` keys". В противном случае ответ может отсутствовать, и вы получите ошибку `400 status code (no body)`.
- Ответ будет представлять собой JSON-строку. Чтобы использовать её структурированное содержимое, вам потребуется сначала разобрать её с помощью других узлов **рабочего процесса**. Вы также можете воспользоваться функцией [Структурированный вывод](/ai-employees/workflow/nodes/llm/structured-output).

## Сообщения

Массив сообщений, отправляемых LLM-модели, может включать набор исторических сообщений. Сообщения поддерживают три типа:

- System (Система) — обычно используется для определения роли и поведения LLM-модели в диалоге.
- User (Пользователь) — содержимое, введённое пользователем.
- Assistant (Ассистент) — содержимое, сгенерированное моделью в ответ.

Для пользовательских сообщений, при условии поддержки моделью, вы можете добавить несколько фрагментов содержимого в один промпт, что соответствует параметру `content`. Если используемая вами модель поддерживает параметр `content` только в виде строки (это относится к большинству моделей, не поддерживающих мультимодальные диалоги), пожалуйста, разделите сообщение на несколько промптов, каждый из которых содержит только один фрагмент содержимого. Таким образом, узел отправит содержимое в виде строки.

![](https://static-docs.nocobase.com/202503041016140.png)

В содержимом сообщений вы можете использовать переменные для ссылки на контекст **рабочего процесса**.

![](https://static-docs.nocobase.com/202503041017879.png)

## Использование содержимого ответа LLM-узла

Вы можете использовать содержимое ответа LLM-узла в качестве переменной в других узлах.

![](https://static-docs.nocobase.com/202503041018508.png)