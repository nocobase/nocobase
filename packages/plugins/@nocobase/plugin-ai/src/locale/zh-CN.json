{
  "AI integration": "AI 集成",
  "LLM services": "LLM 服务",
  "LLM service": "LLM 服务",
  "Model": "模型",
  "UID": "唯一标识",
  "Provider": "LLM 类型",
  "Messages": "消息",
  "Structured output": "结构化输出",
  "Message": "消息",
  "Role": "角色",
  "Add content": "添加内容",
  "Add prompt": "添加提示",
  "Text": "文本",
  "Image": "图片",
  "Timout (ms)": "超时时间（毫秒）",
  "Max retries": "最大重试次数",
  "Frequency penalty description": "介于 -2.0 和 2.0 之间的数字。如果该值为正，那么新 token 会根据其在已有文本中的出现频率受到相应的惩罚，降低模型重复相同内容的可能性。",
  "Max completion tokens description": "限制一次请求中模型生成 completion 的最大 token 数。输入 token 和输出 token 的总长度受模型的上下文长度的限制。",
  "Presence penalty description": "介于 -2.0 和 2.0 之间的数字。如果该值为正，那么新 token 会根据其是否已在已有文本中出现受到相应的惩罚，从而增加模型谈论新主题的可能性。",
  "Response format description": "使用 JSON 模式时，你还必须通过系统或用户消息指示模型生成 JSON。",
  "Temperature description": "采样温度，介于 0 和 2 之间。更高的值，如 0.8，会使输出更随机，而更低的值，如 0.2，会使其更加集中和确定。",
  "Top P description": "作为调节采样温度的替代方案，模型会考虑前 top_p 概率的 token 的结果。所以 0.1 就意味着只有包括在最高 10% 概率中的 token 会被考虑。",
  "Get models list failed, you can enter a model name manually.": "获取模型列表失败，你可以手动输入模型名称。"
}
