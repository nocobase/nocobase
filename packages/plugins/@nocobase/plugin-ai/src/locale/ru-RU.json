{
  "AI integration": "Интеграция с ИИ",
  "LLM services": "Сервисы LLM",
  "LLM service": "Сервис LLM",
  "Model": "Модель",
  "Messages": "Сообщения",
  "Structured output": "Структурированный вывод",
  "Message": "Сообщение",
  "Role": "Роль",
  "UID": "UID",
  "Add content": "Добавить контент",
  "Add prompt": "Добавить запрос",
  "Provider": "Провайдер",
  "Text": "Текст",
  "Image": "Изображение",
  "Timout (ms)": "Таймаут (мс)",
  "Max retries": "Максимум попыток",
  "Frequency penalty description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены на основе их частоты в тексте, уменьшая вероятность повторения одних и тех же строк дословно.",
  "Max completion tokens description": "Верхняя граница количества токенов, которые могут быть сгенерированы для завершения, включая видимые выходные токены и токены рассуждений.",
  "Presence penalty description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены на основе того, появляются ли они в тексте до сих пор, увеличивая вероятность того, что модель будет говорить о новых темах.",
  "Response format description": "Важно: при использовании режима JSON вы также должны указать модели, чтобы она генерировала JSON, через системное или пользовательское сообщение.",
  "Temperature description": "Температура выборки, от 0 до 2. Более высокие значения, такие как 0.8, делают вывод более случайным, а более низкие, такие как 0.2, делают его более сфокусированным и детерминированным.",
  "Top P description": "Альтернатива выборке по температуре, называемая выборкой по ядру, при которой модель учитывает результаты токенов с вероятностью top_p. Так, 0.1 означает, что учитываются только токены, составляющие верхние 10% вероятности.",
  "Get models list failed, you can enter a model name manually.": "Не удалось получить список моделей, вы можете ввести имя модели вручную."
}
